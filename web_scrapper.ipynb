{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26706/181346014.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "import lxml\n",
    "# import functions.web_scrapper_functions as scrpr\n",
    "# import mysql.connector\n",
    "import json\n",
    "# import datetime\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.manto.ae/collections/manto-men\"\n",
    "\n",
    "r = requests.get(url)\n",
    "s = bs4.BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jazba (Passion) - Green Weaved Stole\n",
      "Kaavish (Efforts) - Green & Brown Overshirt\n",
      "Sage Green Lucknow (Men)\n",
      "Zamana (Era Of Dreams) - Forest Green Vasket\n",
      "Zipper Hoodie - French Vanilla\n",
      "Zamana (Era of Dreams) - Black and Olive Green Cape\n",
      "Firdous Set - Men\n",
      "Layla Majnun Pocket Squares (Set of 5)\n",
      "Kaavish (Efforts) - Green & Brown Vasket\n",
      "Jet Black - Lucknow (Men)\n",
      "Pullover Hoodie - Ash Lavender\n",
      "Karvaan (Explorers) - Teal Blue Stole\n",
      "360Â° Tee (Men) - Autumn Blaze\n",
      "Ishq (Love) - Maroon and Beige Cape\n",
      "Zipper Hoodie - Raven Black\n",
      "Junoon (Passion) - Black Stole\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,16):\n",
    "    print(s.find(id=\"root\").select('.product-item__meta')[i].select('a')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dhs. 80.00\n",
      "Dhs. 80.00\n",
      "Dhs. 145.00\n",
      "Dhs. 145.00\n",
      "Dhs. 120.00\n",
      "Dhs. 120.00\n",
      "Dhs. 130.00\n",
      "Dhs. 130.00\n",
      "Dhs. 110.00\n",
      "Dhs. 110.00\n",
      "Dhs. 100.00\n",
      "Dhs. 100.00\n",
      "Dhs. 150.00\n",
      "Dhs. 150.00\n",
      "Dhs. 65.00\n",
      "Dhs. 65.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,16):\n",
    "    print(s.find(id=\"root\").select('.product-item__price')[i].text.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.manto.ae/collections/manto-men?page=7\"\n",
    "\n",
    "r = requests.get(url)\n",
    "s = bs4.BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.product-item__meta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(0,16):\n",
    "    print(s.find(id=\"root\").select('.product-item__meta')[i].select('a')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No products found on page 6 for Men. Exiting.\n",
      "No products found on page 18 for Women. Exiting.\n",
      "Data has been saved to scraped_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import csv\n",
    "\n",
    "# Function to scrape and save data for a given URL and category\n",
    "def scrape_and_save(url, category, data_dict):\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        current_url = url.format(page_number)\n",
    "        r = requests.get(current_url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page_number} for {category}. Exiting.\")\n",
    "            break\n",
    "\n",
    "        s = bs4.BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "        # Check if there are no products on the page\n",
    "        products = s.find(id=\"root\").select('.product-item__meta')\n",
    "        if not products:\n",
    "            print(f\"No products found on page {page_number} for {category}. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # Iterate through all products on the page\n",
    "        for i, product in enumerate(products):\n",
    "            product_name = product.select('a')[0].text\n",
    "            product_price = s.find(id=\"root\").select('.product-item__price')[i].text.replace('\\n', '')\n",
    "\n",
    "            # Store the data in the dictionary\n",
    "            data_dict.append({'Category': category, 'Product': product_name, 'Price': product_price})\n",
    "\n",
    "        # Move on to the next page\n",
    "        page_number += 1\n",
    "\n",
    "# List of URLs and categories\n",
    "urls_and_categories = [\n",
    "    {\"url\": \"https://www.manto.ae/collections/manto-men?page={}\", \"category\": \"Men\"},\n",
    "    {\"url\": \"https://www.manto.ae/collections/best-seller-women?page={}\", \"category\": \"Women\"}\n",
    "]\n",
    "\n",
    "# List to store data\n",
    "data_list = []\n",
    "\n",
    "# Scrape data for each URL and category\n",
    "for item in urls_and_categories:\n",
    "    scrape_and_save(item[\"url\"], item[\"category\"], data_list)\n",
    "\n",
    "# Save data to CSV\n",
    "csv_file_path = \"scraped_data.csv\"\n",
    "fields = ['Category', 'Product', 'Price']\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_list)\n",
    "\n",
    "print(f\"Data has been saved to {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mantoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
